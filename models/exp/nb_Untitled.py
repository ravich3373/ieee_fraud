
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/Untitled.ipynb

%reload_ext autoreload
%autoreload 2
%matplotlib inline

import pandas as pd
from functools import partial
import pdb
import pickle
from collections import defaultdict
from sklearn.model_selection import train_test_split

import numpy as np
import gc
from tqdm import tqdm_notebook as tqdm

import torch
import torch.nn as nn
from torch.utils.data import TensorDataset,DataLoader
from torch.optim import Adam
from torch import tensor

import matplotlib.pyplot as plt

data_pth = '../data/'

trn_id = pd.read_csv(data_pth+'train_identity.csv',index_col='TransactionID')

from typing import Iterable

def listify(p=None):
    if p is None: p=[]
    elif isinstance(p, str):          p = [p]
    elif not isinstance(p, Iterable): p = [p]
    return p

class DfHandler():
    def __init__(self,df, cont=None, cat=None,dep_var=None):
        '''
        creates cont and cat variable lists
        '''
        self.cont = listify(cont)
        self.cat = listify(cat)
        self.dep_var = dep_var
        if self.cont:
            self.cat = set(list(df.columns)) - set(self.cont)
            self.cat = list(self.cat)
        elif self.cat:
            self.cont = set(list(df.columns)) - set(self.cat)
            self.cont = list(self.cont)
        if dep_var in self.cat:
            self.cat.remove(dep_var)
        elif dep_var in self.cont:
            self.cont.remove(dep_var)
        assert (len(df.columns) == len(self.cat)+len(self.cont))+1, "some columns are missed"

    def parse_frame(self,df):
        '''
        1
        calculate features dictionary, for cont features
        we get mean and std, for cat features we store all possible values
        of cateogery.
        '''
        self.feat_dist = {}
        for col in df.columns:
            if col in self.cat:
                self.feat_dist[col] = np.array(df[col].dropna().unique())
            elif col in self.cont:
                self.feat_dist[col] = np.array([df[col].mean(),df[col].std()])
                if (df[col].std() != df[col].std()):
                    print('std is nan')

    def add_nan_cols(self,df):
        '''
        2
        if a  cloumn has Nan in it create a new column next to it
        value is True if Nan else False
        '''
        cols_2_probe = list(df.columns.values)
        cols_2_probe.remove(self.dep_var)
        len_df = 0
        while len(df.columns)-1 == 2*len(cols_2_probe):
            for col in cols_2_probe:
                df.insert(self.get_loc(df,col)+1,col+'_isnan',df[col].isna())
                break
        self.cat.extend([col+'_isnan' for col in df.columns if col+'_isnan' in df.columns])
        return df

    def get_loc(self,df,col):
        '''
        get the location of the given column.
        '''
        for idx,col_ in enumerate(df.columns):
            if col_ == col:
                return idx

    def replace_nans(self,df):
        '''
        3
        replace NaNs in a DataFrame with stats taken during parse
        check if the changes are being reflected in the df
        '''
        for col in df.columns:
            if df[col].isna().sum()>0:
                if col in self.cat:
                    no_nan_feat = len(self.feat_dist[col])
                    if df[col].isna().sum() > 0:
                        if no_nan_feat == 1:
                            df[col][df[col+'_isnan']] = self.feat_dist[col][0]
                        else:
                            df[col][df[col+'_isnan']] = self.feat_dist[col][np.random.randint(0,no_nan_feat-1,df[col+'_isnan'].sum())]
                elif col in self.cont:
                    no_nan_feat = len(self.feat_dist[col])
                    m = self.feat_dist[col][0]
                    s = self.feat_dist[col][1]
                    df[col][df[col+'_isnan']] = np.random.uniform(-1,1,df[col+'_isnan'].sum())*s + m
        return df

    def to_cats(self,df):
        '''
        4
        get all cat vars and convert to cat dtype
        '''
        self.col_2_cat = {} #columns to category number map
        for col in self.cat:
            df[col] = df[col].astype('category').cat.as_ordered()
            self.col_2_cat[col] = df[col].cat.categories
            df[col] = df[col].cat.codes
        return df

    def cat_cont_y_sep(self,df,dep_var):
        '''
        5
        sep cat, cont feat to feed to the model
        '''
        if dep_var in self.cat:
            self.cat.remove(dep_var)
        elif dep_var in self.cont:
            self.cont.remove(dep_var)
        a,b = df.loc[:,self.cat].values, df.loc[:,self.cont].values
        # patch to work with both trn and tst df, as tst has no dep_var col
        if dep_var in df.columns:
            y = df.loc[:,dep_var].values
        else:
            y = [0]*len(df)
        a,b,y = torch.tensor(a,dtype=torch.long),torch.tensor(b,dtype=torch.float),torch.tensor(y,dtype=torch.float)
        return a,b,y

    def get_dataset(self,df,dep_var):
        '''6
        ds from df'''

        cat_dat, cont_dat,y = self.cat_cont_y_sep(df,dep_var)
        ds = TensorDataset(cat_dat,cont_dat,y)
        return ds

    def get_dl(self,ds,bs,shuffle,num_workers,**kwargs):
        '''7
        dl from ds'''
        return DataLoader(ds,bs,shuffle,num_workers=num_workers,**kwargs)

    def norm_df(self,df):
        df_ = df.copy()
        for col in self.cont:
            df_[col] = (df[col] - self.feat_dist[col][0])/self.feat_dist[col][1]
        return df_

    def check_cols(self,col_trn,col_tst):
        '''return True if both col names are exactly same'''
        for i in range(len(col_trn.columns)):
            if col_trn.columns[i] != col_tst.columns[i]:
                return False
        return True

    def prep_tst_df(self,df_tst,df_trn,cat_cols,cont_cols):
        # continue here
        '''
        1) is null cols
        normalizing
        cats handling
        '''
        # add just as many is_null cols at the same pos as in train ds
        i = 0
        while not self.check_cols(df_trn,df_tst) :
            i += 1
            if(i>600):
                print("error: more than 800 iters to work on tst_df")
                break
            for i,col in enumerate(df_trn.columns):
                if df_tst.columns[i] != col:
                    df_tst.insert(i,col,df_tst[col[:-6]].isna())
                    break
        # cats handling
        for col in cat_cols:
            df_tst[col].apply(lambda x: np.where(self.col_2_cat[col] == x)[0])
        # normalize cont vars
        for col in cont_cols:
            df_tst[col] = (df_tst[col] - self.feat_dist[col][0])/self.feat_dist[col][1]

        return df_tst

class PtrnRecog():
    def __init__(self,df,cont=None,cat=None,dep_var=None):
        self.dep_var = dep_var
        self.dfh = DfHandler(df,cont,cat,dep_var)
        # calc distrubutions of features and add isnan fetures
        self.dfh.parse_frame(df)
        #pdb.set_trace()
        self.df = self.dfh.add_nan_cols(df)

        self.cont = self.dfh.cont
        self.cat = self.dfh.cat

        self.lf = nn.MSELoss()
        self.losses,self.losses_val = [],[]

    def get_data_ready(self,bs,shuffle,num_workers,**kwargs):
        self.bs = bs
        self.shuffle = shuffle
        self.num_workers = num_workers
        self.kwargs = kwargs
        self.df = self.dfh.replace_nans(self.df)
        pdb.set_trace()
        self.df = self.dfh.to_cats(self.df)
        #self.dfn = self.dfh.norm_df(self.df)
        # repeat work being done in shuffle_nans
        #self.ds = self.dfh.get_dataset(self.dfn,self.dep_var)
        #self.dl = self.dfh.get_dl(self.ds,self.bs,self.shuffle,self.num_workers,**self.kwargs)

    def shuffle_nans(self):
        self.df = self.dfh.replace_nans(self.df)
        self.dfn = self.dfh.norm_df(self.df) # df normalized

        self.dfn_trn,self.dfn_val = train_test_split(self.dfn,test_size=0.3)

        self.ds_trn = self.dfh.get_dataset(self.dfn_trn,self.dep_var)
        self.ds_val = self.dfh.get_dataset(self.dfn_val,self.dep_var)

        self.dl_trn = self.dfh.get_dl(self.ds_trn,self.bs,self.shuffle,self.num_workers,**self.kwargs)
        self.dl_val = self.dfh.get_dl(self.ds_val,self.bs*4,self.shuffle,self.num_workers,**self.kwargs)

    def get_model(self,h1,h2):
        self.m = TabularNN(self.df,self.cat,len(self.cont),h1,h2)

    def get_opt(self):
        self.opt = Adam(self.m.parameters())

    def fit(self):
        #self.losses = []
        self.m.train()
        for d_cat,d_cont,y in tqdm(self.dl_trn):
            if self.gpu:
                d_cat,d_cont,y = d_cat.cuda(),d_cont.cuda(),y.cuda()
            self.opt.zero_grad()
            preds_b = self.m(d_cat,d_cont,y)
            loss = self.lf(preds_b,y)
            loss.backward()
            self.opt.step()
            self.losses.append(loss.item())

        self.m.eval()
        with torch.no_grad():
            for d_cat,d_cont,y in tqdm(self.dl_val):
                if self.gpu:
                    d_cat,d_cont,y = d_cat.cuda(),d_cont.cuda(),y.cuda()
                preds_b = self.m(d_cat,d_cont,y)
                loss = self.lf(preds_b,y)
                self.losses_val.append(loss.item())

    def predict(self,df):
        self.makeup_test_df(df)
        preds = []
        for d_cat,d_cont,y in tqdm(self.dl_tst):
            if self.gpu:
                d_cat,d_cont,y = d_cat.cuda(),d_cont.cuda(),y.cuda()

                preds_b = self.m(d_cat,d_cont,y)
                preds.append(preds_b)
        return preds

    def learn(self,epochs):
        for epoch in range(epochs):
            print(epoch)
            self.shuffle_nans()
            self.fit()

    def to_gpu(self):
        self.gpu = True
        self.m.cuda()

    def makeup_test_df(self,df):
        self.df_tst =  self.dfh.prep_tst_df(df,self.df,self.cat,self.cont)
        self.ds_tst = self.dfh.get_dataset(self.df_tst,self.dep_var)
        self.dl_tst = self.dfh.get_dl(self.ds_tst,self.bs*4,False,self.num_workers)#shuffle,num_workers

class RunningBatchNorm(nn.Module):
    def __init__(self, nf, mom=0.1, eps=1e-5):
        super().__init__()
        self.mom, self.eps = mom, eps
        self.mults = nn.Parameter(torch.ones (nf,1,1))
        self.adds  = nn.Parameter(torch.zeros(nf,1,1))
        self.register_buffer('sums', torch.zeros(1,nf,1,1))
        self.register_buffer('sqrs', torch.zeros(1,nf,1,1))
        self.register_buffer('count', tensor(0.))
        self.register_buffer('factor', tensor(0.))
        self.register_buffer('offset', tensor(0.))
        self.batch = 0

    def update_stats(self, x):
        bs,nc,*_ = x.shape
        self.sums.detach_()
        self.sqrs.detach_()
        dims = (0,2,3)
        s    = x    .sum(dims, keepdim=True)
        ss   = (x*x).sum(dims, keepdim=True)
        c    = s.new_tensor(x.numel()/nc)
        mom1 = s.new_tensor(1 - (1-self.mom)/math.sqrt(bs-1))
        self.sums .lerp_(s , mom1)
        self.sqrs .lerp_(ss, mom1)
        self.count.lerp_(c , mom1)
        self.batch += bs
        means = self.sums/self.count
        varns = (self.sqrs/self.count).sub_(means*means)
        if bool(self.batch < 20): varns.clamp_min_(0.01)
        self.factor = self.mults / (varns+self.eps).sqrt()
        self.offset = self.adds - means*self.factor

    def forward(self, x):
        if self.training: self.update_stats(x)
        return x*self.factor + self.offset

class TabularNN(nn.Module):
    def __init__(self,df,cat_cols, cont_featsz, h1,h2):
        super().__init__()
        self.nuniq = df[cat_cols].nunique().values
        self.emb_szs = np.array(list(map(lambda x: min(50, round(1.6* x**0.56)), self.nuniq)), dtype=int)
        #print(self.nuniq,self.emb_szs)
        self.embs = nn.ModuleList([nn.Embedding(n,sz) for n,sz in zip(self.nuniq,self.emb_szs)])

        self.f_sz = 0
        for sz in self.emb_szs:
            self.f_sz += sz
        self.f_sz += cont_featsz

        self.rbn1 = RunningBatchNorm(self.f_sz)
        self.l1 = nn.Linear(self.f_sz,h1)
        self.nl1 = nn.ReLU()
        self.rbn2 = RunningBatchNorm(h1)
        self.l2 = nn.Linear(h1,h2)
        self.nl2 = nn.ReLU()
        self.rbn3 = RunningBatchNorm(h2)
        self.f1 = nn.Linear(h2,1)
        self.sig = nn.Sigmoid()

    def forward(self,d_cat,d_cont,y):
        x = [e(d_cat[:,i]) for i,e in enumerate(self.embs)]
        x.append(d_cont)
        x = torch.cat(x,dim=1)
        x = self.nl1(self.l1(x))
        x = self.nl2(self.l2(x))
        x = self.f1(x)
        x = self.sig(x)
        return x.squeeze(1)

class Hook():
    def __init__(self, m, f): self.hook = m.register_forward_hook(partial(f, self))
    def remove(self): self.hook.remove()
    def __del__(self): self.remove()

def append_stats(hook, mod, inp, outp):
    #pdb.set_trace()
    if not hasattr(hook,'stats'): hook.stats = ([],[])
    means,stds = hook.stats
    if mod.training:
        means.append(outp.data.mean())
        stds .append(outp.data.std())

class ListContainer():
    def __init__(self, items): self.items = listify(items)
    def __getitem__(self, idx):
        try: return self.items[idx]
        except TypeError:
            if isinstance(idx[0],bool):
                assert len(idx)==len(self) # bool mask
                return [o for m,o in zip(idx,self.items) if m]
            return [self.items[i] for i in idx]
    def __len__(self): return len(self.items)
    def __iter__(self): return iter(self.items)
    def __setitem__(self, i, o): self.items[i] = o
    def __delitem__(self, i): del(self.items[i])
    def __repr__(self):
        res = f'{self.__class__.__name__} ({len(self)} items)\n{self.items[:10]}'
        if len(self)>10: res = res[:-1]+ '...]'
        return res

class Hooks(ListContainer):
    def __init__(self, ms, f):
        ms = list(ms.children())[1:]
        super().__init__([Hook(m, f) for m in ms])
    def __enter__(self, *args): return self
    def __exit__ (self, *args): self.remove()
    def __del__(self): self.remove()

    def __delitem__(self, i):
        self[i].remove()
        super().__delitem__(i)

    def remove(self):
        for h in self: h.remove()